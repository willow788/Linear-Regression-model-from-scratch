<div align="center">

# ğŸ¯ Linear Regression from Scratch

### *Building Machine Learning Foundations, One Gradient at a Time*

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Jupyter](https://img.shields.io/badge/Jupyter-Notebook-orange.svg)](https://jupyter.org/)
[![NumPy](https://img.shields.io/badge/NumPy-Latest-013243.svg)](https://numpy.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Scikit-Learn](https://img.shields.io/badge/Scikit--Learn-Validation-F7931E.svg)](https://scikit-learn.org/)

*A comprehensive implementation of Linear Regression with multiple gradient descent methods, polynomial features, and L1 regularization*

[Features](#-features) â€¢ [Installation](#-installation) â€¢ [Usage](#-usage) â€¢ [Project Structure](#-project-structure) â€¢ [Experiment Logs](#-experiment-logs)

</div>

---

## ğŸ“– About The Project

This repository contains a **from-scratch implementation** of Linear Regression, built to deeply understand the mathematics and mechanics behind one of the most fundamental machine learning algorithms.  

### ğŸ“ What Makes This Special? 

- âœ… **Pure NumPy Implementation** - No black-box ML libraries for core algorithm
- âœ… **Three Gradient Descent Methods** - Batch, Stochastic, and Mini-Batch
- âœ… **Polynomial Feature Engineering** - Up to 2nd degree with interaction terms
- âœ… **L1 Regularization (Lasso)** - Prevent overfitting and feature selection
- âœ… **Early Stopping** - Intelligent training termination
- âœ… **K-Fold Cross-Validation** - Robust model evaluation
- âœ… **Comprehensive Visualizations** - Loss curves, residuals, correlations
- âœ… **Detailed Experiment Logs** - Journey from negative RÂ² to 98%+ accuracy

---

## ğŸš€ Features

### ğŸ§® Multiple Gradient Descent Methods

<table>
<tr>
<td width="33%" align="center">
<h4>Batch Gradient Descent</h4>
<p>Uses entire dataset per iteration</p>
<p>âœ… Stable convergence</p>
<p>âœ… Smooth loss curves</p>
</td>
<td width="33%" align="center">
<h4>Stochastic Gradient Descent</h4>
<p>One sample at a time</p>
<p>âœ… Fast updates</p>
<p>âœ… Escapes local minima</p>
</td>
<td width="33%" align="center">
<h4>Mini-Batch GD</h4>
<p>Best of both worlds</p>
<p>âœ… Balanced speed</p>
<p>âœ… Memory efficient</p>
</td>
</tr>
</table>

### ğŸ“Š Advanced Features

| Feature | Description | Benefit |
|---------|-------------|---------|
| **Polynomial Features** | TVÂ², RadioÂ², TVÃ—Radio, etc. | Captures non-linear relationships |
| **L1 Regularization** | Lasso penalty on weights | Prevents overfitting, feature selection |
| **Z-Score Normalization** | Standardizes features and targets | Faster convergence, stable gradients |
| **Early Stopping** | Monitors loss with patience | Prevents unnecessary iterations |
| **K-Fold CV** | 5-fold cross-validation | Robust performance estimation |

---

## ğŸ“¦ Installation

### Prerequisites

```bash
Python 3.8+
pip package manager
```

### Quick Start

```bash
# Clone the repository
git clone https://github.com/willow788/Linear-Regression-model-from-scratch.git

# Navigate to project directory
cd Linear-Regression-model-from-scratch

# Install dependencies
pip install numpy pandas scikit-learn matplotlib seaborn
```

### Dependencies

```python
numpy>=1.19.0
pandas>=1.1.0
scikit-learn>=0.23.0
matplotlib>=3.3.0
seaborn>=0.11.0
```

---

## ğŸ¯ Usage

### Quick Example

```python
from linear_regression import LinearRegression
from data_preprocessing import load_and_preprocess_data

# Load and preprocess data
X_train, X_test, y_train, y_test = load_and_preprocess_data('Advertising.csv')

# Initialize model
model = LinearRegression(
    learn_rate=0.02,
    iter=50000,
    method='batch',
    l1_reg=0.1
)

# Train the model
model.fit(X_train, y_train)

# Make predictions
predictions = model.predict(X_test)
```

### Running the Complete Pipeline

```bash
# Run the main script
python main.py
```

### Jupyter Notebook Exploration

```bash
# Launch Jupyter
jupyter notebook

# Open any version notebook
# Navigate to Version- 9/Raw jupyter Notebook/sales. ipynb
```

---

## ğŸ“ Project Structure

```
Linear-Regression-model-from-scratch/
â”‚
â”œâ”€â”€ ğŸ“‚ Version- 1/                    # Initial experiments
â”‚   â””â”€â”€ experiment_log.txt            # Detailed notes on failures and learnings
â”‚
â”œâ”€â”€ ğŸ“‚ Version- 2/                    # Feature engineering experiments
â”‚   â””â”€â”€ experiment_log.txt
â”‚
â”œâ”€â”€ ğŸ“‚ Version- 3/                    # Normalization improvements
â”‚   â””â”€â”€ experiment_log.txt
â”‚
â”œâ”€â”€ ğŸ“‚ Version- 9/                    # Final optimized version
â”‚   â”œâ”€â”€ Raw jupyter Notebook/
â”‚   â”‚   â””â”€â”€ sales.ipynb              # Complete analysis notebook
â”‚   â””â”€â”€ Python Files/
â”‚       â”œâ”€â”€ data_preprocessing.py    # Data loading and feature engineering
â”‚       â”œâ”€â”€ linear_regression.py     # Core model implementation
â”‚       â”œâ”€â”€ model_evaluation.py      # Metrics and cross-validation
â”‚       â”œâ”€â”€ visualization.py         # Plotting utilities
â”‚       â”œâ”€â”€ main.py                  # Main execution script
â”‚       â””â”€â”€ config.py                # Configuration parameters
â”‚
â”œâ”€â”€ ğŸ“Š Advertising.csv                # Dataset
â””â”€â”€ ğŸ“– README.md
```

---

## ğŸ§ª Experiment Logs

<details>
<summary><b>ğŸ”´ Version 1: The Negative RÂ² Crisis</b></summary>

### Problem
- **RÂ² Score: -18.77** ğŸ˜±
- Model performing worse than predicting mean

### Root Causes Discovered
1. No feature normalization
2. Learning rate too high causing divergence
3. Basic linear features insufficient for non-linear relationships

### Key Learnings
> "Sometimes you need to fail spectacularly to understand the fundamentals."

</details>

<details>
<summary><b>ğŸŸ¡ Version 2-3: Feature Engineering Journey</b></summary>

### Experiments Conducted
- Added polynomial features (TVÂ², RadioÂ², NewspaperÂ²)
- Implemented interaction terms (TVÃ—Radio, etc.)
- Introduced Z-score normalization
- Tuned learning rates systematically

### Results
- RÂ² improved to ~0.85
- Still experiencing some instability

</details>

<details>
<summary><b>ğŸŸ¢ Version 9: Production-Ready Model</b></summary>

### Final Optimizations
âœ… **Z-score normalization** for features and target  
âœ… **L1 regularization** (Î» = 0.1-0.2)  
âœ… **Early stopping** with patience = 1000  
âœ… **K-fold cross-validation** for robust evaluation  
âœ… **Multiple GD methods** for comparison  

### Performance Metrics

| Metric | Batch GD | Stochastic GD | Mini-Batch GD |
|--------|----------|---------------|---------------|
| **Test RÂ²** | 0.9584 | 0.9850 | 0.9874 |
| **Train RÂ²** | 0.9509 | 0.9848 | 0.9860 |
| **RMSE** | 0.2249 | 0.1352 | 0.1238 |
| **MAE** | 0.1533 | 0.1118 | 0.1011 |

### ğŸ‰ Best Model:  Mini-Batch GD
- **RÂ² Score: 98.74%**
- **Batch Size: 16**
- **Learning Rate: 0.01**
- **Iterations: 1000**

</details>

---

## ğŸ“Š Visualizations

<div align="center">

### Loss Convergence

The model demonstrates smooth convergence with proper hyperparameters

### Residual Analysis

Residuals show random scatter around zero, indicating good model fit

### Feature Importance

TV advertising shows strongest correlation with sales, followed by Radio

</div>

---

## ğŸ”¬ Mathematical Foundation

### Linear Regression Equation

$$\hat{y} = X\mathbf{w} + b$$

### Loss Function (with L1 Regularization)

$$L(\mathbf{w}, b) = \frac{1}{2m}\sum_{i=1}^{m}(h_\mathbf{w}(x^{(i)}) - y^{(i)})^2 + \frac{\lambda}{2}\sum_{j=1}^{n}|w_j|$$

### Gradient Descent Update Rules

$$\mathbf{w} := \mathbf{w} - \alpha \cdot \frac{1}{m}X^T(X\mathbf{w} - \mathbf{y}) - \alpha \cdot \lambda \cdot \text{sign}(\mathbf{w})$$

$$b := b - \alpha \cdot \frac{1}{m}\sum_{i=1}^{m}(h_\mathbf{w}(x^{(i)}) - y^{(i)})$$

Where: 
- $\alpha$ = learning rate
- $m$ = number of samples
- $\lambda$ = regularization parameter

---

## ğŸ“ˆ Dataset

**Advertising Dataset**
- **Source**: Kaggle/UCI ML Repository
- **Samples**: 200
- **Features**: TV, Radio, Newspaper advertising budgets
- **Target**: Sales figures

### Feature Engineering

Original 3 features expanded to 9:
1. TV
2. Radio
3. Newspaper
4. TVÂ² (squared term)
5. RadioÂ² (squared term)
6. NewspaperÂ² (squared term)
7. TV Ã— Radio (interaction)
8. TV Ã— Newspaper (interaction)
9. Radio Ã— Newspaper (interaction)

---

## ğŸ“ Key Learnings

### 1. **Data Normalization is Critical**
Without normalization, gradients explode and convergence fails

### 2. **Feature Engineering Matters**
Polynomial and interaction terms capture non-linear relationships

### 3. **Regularization Prevents Overfitting**
L1 penalty keeps weights small and performs feature selection

### 4. **Hyperparameter Tuning is an Art**
Learning rate, regularization, and batch size must be balanced

### 5. **Cross-Validation is Essential**
K-fold CV provides honest performance estimates

---

## ğŸ› ï¸ Future Improvements

- [ ] Add Elastic Net (L1 + L2)
- [ ] Adaptive learning rates (Adam, RMSprop)
- [ ] Automatic hyperparameter tuning (Grid Search)
- [ ] Feature selection algorithms
- [ ] Support for categorical features
- [ ] Model serialization (save/load)
- [ ] Web interface for predictions

---

## ğŸ¤ Contributing

Contributions are welcome! Feel free to: 

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- **Dataset**: Advertising dataset from Kaggle
- **Inspiration**: Andrew Ng's Machine Learning course
- **Libraries**: NumPy, Pandas, Matplotlib, Seaborn, Scikit-Learn

---

<div align="center">

### â­ Star this repo if you found it helpful! 

**Built with ğŸ’™ and â˜• by [willow788](https://github.com/willow788)**

*Learning by doing, one line of code at a time*

</div>
